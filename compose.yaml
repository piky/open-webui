services:
  open-webui:
    container_name: open-webui
    image: ghcr.io/open-webui/open-webui:ollama
    volumes:
      - open-webui:/app/backend/data
      - ollama:/root/.ollama
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/"] # Or a more specific endpoint like /health if available
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s # Give the container some time to start before checking

  caddy:
    image: caddy:latest
    restart: unless-stopped
    environment:
      - PUBLIC_HOSTNAME=${PUBLIC_HOSTNAME:-localhost}   # Public hostname that is configured in CloudFlare Tunnels
      - BACKEND_HOSTNAME=${BACKEND_HOSTNAME:-open-webui}
      - BACKEND_PORT=${BACKEND_PORT:-8080}
    volumes:
      # - ./conf:/etc/caddy   # Uncomment this line to use custom Caddyfile 
      - ./site:/srv
      - caddy_data:/data
      - caddy_config:/config
    command: caddy reverse-proxy --from ${PUBLIC_HOSTNAME}:80 --to ${BACKEND_HOSTNAME}:${BACKEND_PORT}
    depends_on:
      open-webui:
        condition: service_healthy

  cloudflared:
    image: cloudflare/cloudflared:latest
    restart: unless-stopped
    command: tunnel --no-autoupdate run
    environment:
      - TUNNEL_TOKEN=${TUNNEL_TOKEN}
    depends_on:
      open-webui:
        condition: service_healthy

volumes:
  open-webui:
  ollama:
  caddy_data:
  caddy_config: